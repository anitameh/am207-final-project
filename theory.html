<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Mobility in the U.S.A.</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h2>MOBILITY IN THE U.S.A.: A BAYESIAN PERSPECTIVE ON A DIFFICULT QUESTION</h2>

        <p class="view"><a href="https://github.com/anitameh/am207-final-project">View the Project on GitHub</a></p>

        <ul>
          <li><a href="index.html">Home</a></li>
          <li><a href="figures.html">Results & Figures </a></li>
          <li><a href="screencast.html">Screencast</a></li>
        </ul>

      </header>
      <section>
      <p><b>Probabilistic Graphical Models: A Brief Introduction</b></p>
      <p>
          Probabilistic graphical models (PGMs) provide a mechanism for exploiting structure in complex distributions, in order to describe them compactly. By using conditional independence assumptions, we are able to construct and effectively represent joint distributions. PGMs are commonly known as ``Bayesian networks" (though this name can be misleading).
      </p>
      <p>
          "Hierarchical models" are a type of Bayesian network, though there does not exist a consistent, single definition. In this paper, we use the definition that refers to a type of multi-level model where parameters are nested within another. In the next section, we provide a high-level overview of partial pooling in the context of hierarchical models.
      </p>
      <p><b>Hierarchical Models and Pooling</b></p>
      <p>
        When modeling a data set, the concept of "pooling" often arises, though in many cases, implicitly. "Complete pooling" in, for example, a linear regression, is perhaps the most common case because it makes the assumption that all data points from all experiments come from the same parameter; that is, they are completely pooled together. For our data set, we can visualize this model thusly:
        <center>
          <img class="" src="complete-pooling.jpg" style="width:device-width;max-width:70%;margin:auto;display:inline"/>
        </center>
      </p>  
      <p>
        "No pooling," on the other hand, is the case when each experiment has its own identifying parameter. Neither of these modeling techniques, however, is completely satisfactory: in the first case, the estimated parameters are often under-fit because they are pulled by outliers; in the second case, the estimated parameters are highly over-fit because they rely on a very small number of data points:
        <center>
          <img class="" src="no-pooling.jpg" style="width:device-width;max-width:70%;margin:auto;display:inline"/>
        </center>
      </p>
      <p>
        "Partial pooling" is a compromise between the two, and stems from the idea that though there exist parameters for each experiment, these parameters share information with one another. More specifically, they are drawn from a common distribution. Furthermore, <i>group-level parameters</i> capture information at a group-level:
        <center>
          <img class="" src="partial-pooling.jpg" style="width:device-width;max-width:70%;margin:auto;display:inline"/>
        </center>
        Here, each element y corresponds to the standardized mean child income in county i and state j. Since we do not know the hyperparameters, we can either learn them by performing a full Bayesian treatment (as is done in this project), or conduct Bayesian approximation by doing, for example, moment-matching.
      </p>
        
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/anitameh">anitameh</a></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>